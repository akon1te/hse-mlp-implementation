{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.layers.linear import Linear\n",
    "from src.activation.functional import ReLU, Softmax, MSELoss\n",
    "from src.optim.optim import SGD\n",
    "from src.utils.template import BaseClass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(BaseClass):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.layers = [\n",
    "            Linear(input_size, hidden_size),\n",
    "            ReLU(), \n",
    "            Linear(hidden_size, output_size),\n",
    "            Softmax()\n",
    "        ]\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert len(self.layers) > 0\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size=5, output_size=10, hidden_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0.5, 0.7, 0.3, 0.1, 0.8],\n",
    "                   [0.1, 0.6, 0.9, 0.01, 0.4]])\n",
    "gt = np.array([2, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0811569 , 0.05634413, 0.13046026, 0.08205761, 0.11503573,\n",
       "        0.08182587, 0.12946397, 0.1299204 , 0.11143506, 0.08230008],\n",
       "       [0.08583283, 0.0670484 , 0.11323085, 0.0784106 , 0.13116556,\n",
       "        0.08165283, 0.12084914, 0.13817391, 0.09952953, 0.08410635]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.forward(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 7])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = np.argmax(preds, axis=1)\n",
    "cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_loss = MSELoss()\n",
    "loss = my_loss(cls, gt)\n",
    "loss_grad = my_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (20,1) and (20,1) not aligned: 1 (dim 1) != 20 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m SGD(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_loss\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/hse-mlp-implementation/notebooks/../src/optim/optim.py:21\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, model, loss)\u001b[0m\n\u001b[1;32m     19\u001b[0m error_gradient \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror_gradient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     layer\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m layer\u001b[38;5;241m.\u001b[39mweights_grad\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/hse-mlp-implementation/notebooks/../src/activation/functional.py:37\u001b[0m, in \u001b[0;36mSoftmax.backward\u001b[0;34m(self, gradient)\u001b[0m\n\u001b[1;32m     35\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m grad \u001b[38;5;241m=\u001b[39m gradient\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiagflat(values) \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(grad, d)\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (20,1) and (20,1) not aligned: 1 (dim 1) != 20 (dim 0)"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(learning_rate=0.05)\n",
    "optimizer.step(model, my_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hse-mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
