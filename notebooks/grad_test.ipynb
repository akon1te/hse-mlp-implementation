{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.nn.linear import Linear\n",
    "from src.nn.functional import ReLU, Softmax, MSELoss\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(54)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение аутпутов Forward и Backward методов для всех реализованных слоев с аутпутам torch реализаций "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тесты линейного слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init layers\n",
    "torch_linear = torch.nn.Linear(3, 4, bias=True)\n",
    "\n",
    "weigths = torch_linear.weight.data.numpy()\n",
    "bias = torch_linear.bias.data.numpy()\n",
    "\n",
    "my_linear = Linear(3, 4, bias=True)\n",
    "my_linear.weights = weigths\n",
    "my_linear.bias = bias\n",
    "\n",
    "#test input\n",
    "input_data = np.random.randn(3, 3).astype(np.float32)\n",
    "loss_gradient = np.random.randn(3, 4).astype(np.float32)\n",
    "\n",
    "#forward test\n",
    "my_output = my_linear.forward(input_data)\n",
    "torch_output = torch_linear(torch.tensor(input_data))\n",
    "\n",
    "assert (my_output - torch_output.detach().numpy() < 1e-6).all() == True\n",
    "\n",
    "#backward test\n",
    "my_linear.backward(loss_gradient)\n",
    "torch_output.backward(torch.tensor(loss_gradient))\n",
    "\n",
    "assert (my_linear.weights_grad.T - torch_linear.weight.grad.detach().numpy() < 1e-6).all() == True\n",
    "assert (my_linear.bias_grad.T - torch_linear.bias.grad.detach().numpy() < 1e-6).all() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.8958917   0.82432544 -0.86485815]\n",
      " [ 1.1170831   0.01014531 -1.0646278 ]\n",
      " [-3.3169525  -4.734642    1.0889834 ]\n",
      " [ 4.9491887   3.6359472  -1.2768121 ]]\n",
      "\n",
      "[[ 2.8958917   0.82432544 -0.86485815]\n",
      " [ 1.1170831   0.01014531 -1.0646278 ]\n",
      " [-3.3169525  -4.734642    1.0889834 ]\n",
      " [ 4.9491887   3.6359472  -1.2768121 ]]\n"
     ]
    }
   ],
   "source": [
    "print(my_linear.weights_grad.T, end='\\n\\n')\n",
    "print(torch_linear.weight.grad.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8582057 -1.0067805  2.1648114 -3.0944514]\n",
      "\n",
      "[-1.8582057 -1.0067805  2.1648114 -3.0944514]\n"
     ]
    }
   ],
   "source": [
    "print(my_linear.bias_grad.T, end='\\n\\n')\n",
    "print(torch_linear.bias.grad.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тесты ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init layers\n",
    "my_relu = ReLU()\n",
    "torch_relu = torch.nn.ReLU()\n",
    "\n",
    "#test input\n",
    "input_data = np.random.randn(3, 4).astype(np.float32)\n",
    "torch_input_data = torch.tensor(input_data, requires_grad=True)\n",
    "loss_gradient = np.random.randn(3, 4).astype(np.float32)\n",
    "\n",
    "#forward test\n",
    "my_output = my_relu.forward(input_data)\n",
    "torch_output = torch_relu(torch_input_data)\n",
    "\n",
    "assert (my_output - torch_output.detach().numpy() < 1e-6).all() == True\n",
    "\n",
    "#backward test\n",
    "out_grad = my_relu.backward(loss_gradient)\n",
    "torch_output.backward(torch.tensor(loss_gradient))\n",
    "\n",
    "assert (out_grad - torch_input_data.grad.detach().numpy() < 1e-6).all() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.         -0.         -0.45069039  0.14520353]\n",
      " [ 0.          0.          0.37093243  0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "\n",
      "[[ 0.          0.         -0.4506904   0.14520353]\n",
      " [ 0.          0.          0.37093243  0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(out_grad, end='\\n\\n')\n",
    "print(torch_input_data.grad.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тесты Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test input\n",
    "input_data = np.random.randn(3, 4).astype(np.float32)\n",
    "torch_input_data = torch.tensor(input_data, requires_grad=True)\n",
    "loss_gradient = np.random.randn(3, 4).astype(np.float32)\n",
    "\n",
    "#init layers\n",
    "my_softmax = Softmax()\n",
    "torch_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#forward test\n",
    "my_output = my_softmax.forward(input_data)\n",
    "torch_output = torch_softmax(torch_input_data)\n",
    "\n",
    "assert (my_output - torch_output.detach().numpy() < 1e-6).all() == True\n",
    "\n",
    "#backward test\n",
    "out_grad = my_softmax.backward(loss_gradient)\n",
    "torch_output.backward(torch.tensor(loss_gradient))\n",
    "\n",
    "assert (out_grad - torch_input_data.grad.detach().numpy() < 1e-6).all() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0641047  0.06651817 0.58650583 0.2828713 ]\n",
      " [0.35183212 0.11770014 0.12231466 0.4081531 ]\n",
      " [0.70372367 0.11998278 0.04972024 0.12657331]]\n",
      "\n",
      "[[0.06410469 0.06651816 0.5865058  0.2828713 ]\n",
      " [0.3518321  0.11770012 0.12231466 0.40815306]\n",
      " [0.70372367 0.11998277 0.04972023 0.12657332]]\n"
     ]
    }
   ],
   "source": [
    "print(my_output, end='\\n\\n')\n",
    "print(torch_output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03177635 -0.17044323  0.33679397 -0.13457437]\n",
      " [ 0.47295687  0.04864965  0.00402987 -0.52563636]\n",
      " [-0.06725832 -0.02446857 -0.00678753  0.09851441]]\n",
      "\n",
      "[[-0.03177634 -0.1704432   0.33679393 -0.13457437]\n",
      " [ 0.4729568   0.04864964  0.00402986 -0.5256364 ]\n",
      " [-0.06725833 -0.02446857 -0.00678753  0.09851442]]\n"
     ]
    }
   ],
   "source": [
    "print(out_grad, end='\\n\\n')\n",
    "print(torch_input_data.grad.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тесты MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init layers\n",
    "my_mse = MSELoss()\n",
    "torch_mse = torch.nn.MSELoss()\n",
    "\n",
    "#test input\n",
    "input_data = np.random.randn(3, 4).astype(np.float32) \n",
    "torch_input_data = torch.tensor(input_data, requires_grad=True)\n",
    "labels = np.random.randn(3, 4).astype(np.float32) \n",
    "\n",
    "#forward test\n",
    "my_output = my_mse(input_data, labels)\n",
    "torch_output = torch_mse(torch_input_data, torch.tensor(labels))\n",
    "\n",
    "assert (my_output - torch_output.item() < 1e-6).all() == True\n",
    "\n",
    "#backward test\n",
    "out_grad = my_mse.backward()\n",
    "torch_output.backward()\n",
    "\n",
    "assert (out_grad - torch_input_data.grad.numpy() < 1e-6).all() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0901196\n",
      "\n",
      "2.0901198\n",
      "\n",
      "[[-0.17132546  0.16456425 -0.00071051 -0.12547956]\n",
      " [-0.60560256 -0.07275766  0.4123123  -0.1442021 ]\n",
      " [-0.17743908  0.00645827  0.05119051 -0.1659413 ]]\n",
      "\n",
      "[[-0.17132547  0.16456425 -0.00071051 -0.12547956]\n",
      " [-0.6056026  -0.07275766  0.41231233 -0.14420211]\n",
      " [-0.17743908  0.00645827  0.05119052 -0.1659413 ]]\n"
     ]
    }
   ],
   "source": [
    "print(my_output, end='\\n\\n')\n",
    "print(torch_output.detach().numpy(), end='\\n\\n')\n",
    "\n",
    "print(out_grad, end='\\n\\n')\n",
    "print(torch_input_data.grad.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hse-mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
